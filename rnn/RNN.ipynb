{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "584e1b88-520b-4e5a-b387-3b9798dac4c9",
   "metadata": {},
   "source": [
    "In this notebook, we will implement the forward function of a simple RNN and compare with the PyTorch implementation in torch.nn.RNN to see that we get the same results. First, recall that for an RNN we need to sets of weight matrices and bias vectors - one the get from the input layer to the hidden layer and one to get from the hidden layer of the previous time step to the hidden layer of the current time step. Let us start by fixing the dimensions of the input layer and the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e526ed65-1f2d-47f3-a5a3-de8c704c7c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "d_hidden = 3\n",
    "d_in = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f70af-3bc1-4444-bc2a-fee5a2d688be",
   "metadata": {},
   "source": [
    "The matrix $W_{ih}$ from the inner layer to the hidden layer therefore needs to map from 5 dimensions to 3 dimensions, i.e has shape (3,5). Let us initialize it with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3674dd9d-b303-43fe-b05b-bdfcabf4c12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_ih = torch.randn((d_hidden, d_in))\n",
    "b_ih = torch.randn(d_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d87dea-42ae-4048-9e55-f7c0dba8b5f9",
   "metadata": {},
   "source": [
    "The matrix that we use to map between the values of the hidden layers at different time steps is of course quadratic, let us initialize it as well, along with the respective bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2547897f-c9bf-4b90-9b46-fe943e0a03f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_hh = torch.randn((d_hidden, d_hidden))\n",
    "b_hh = torch.randn(d_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cad8ad-d422-431c-bc6e-13f2fbb2cb41",
   "metadata": {},
   "source": [
    "We can now implement the forward function of the network. Recall that at each time step t, we need to apply the formula\n",
    "$$\n",
    "h_t = tanh(x_t W_{ih}^t + b_{ih} + h_{t-1}W_{hh}^t + b_{hh})\n",
    "$$\n",
    "In addition, we want our forward function to be able to optionally accept the hidden layer from a previous step and to also return the new value of the hidden layer along with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1518263d-3dce-4811-b350-16128216cc0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward(x, h = None):\n",
    "    L = x.shape[0]\n",
    "    if h is None:\n",
    "        h = torch.zeros(d_hidden)\n",
    "    out = []\n",
    "    for t in range(L):\n",
    "      h = torch.tanh(x[t] @ w_ih.t() + b_ih + h @ w_hh.t() + b_hh)\n",
    "      out.append(h)\n",
    "    return torch.stack(out), h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2535a3ca-80d4-492b-9f69-5657769d0e37",
   "metadata": {},
   "source": [
    "Let us run this for a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf64bf9-d178-466b-a395-9dfd93107236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4771,  0.8883,  0.8964],\n",
      "        [ 0.7838, -0.9487, -0.9940],\n",
      "        [ 0.9864, -0.6825, -0.9588],\n",
      "        [ 0.9998, -0.6754,  0.8989],\n",
      "        [ 0.9993,  0.9369, -0.9899]])\n",
      "tensor([ 0.9993,  0.9369, -0.9899])\n"
     ]
    }
   ],
   "source": [
    "L = 5\n",
    "x = torch.randn((L, d_in))\n",
    "out, hidden = forward(x)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d28d1a-d6df-4c21-a0f0-62d995a9b18e",
   "metadata": {},
   "source": [
    "To verify that this is correct, let us compare this to the implementation that comes with PyTorch. For that purpose, we initialize a PyTorch RNN, extract the weights, apply the RNN and our forward function to x and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708cb031-5a5d-4483-8f26-0ea19218ea43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match of outputs: True\n",
      "Match of hidden layers: True\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.RNN(input_size = d_in, hidden_size = d_hidden)\n",
    "w_hh = rnn.weight_hh_l0\n",
    "w_ih = rnn.weight_ih_l0\n",
    "b_ih = rnn.bias_ih_l0\n",
    "b_hh = rnn.bias_hh_l0\n",
    "assert w_hh.shape == (d_hidden, d_hidden)\n",
    "assert w_ih.shape == (d_hidden, d_in)\n",
    "_out, _hidden = rnn(x)\n",
    "out, hidden = forward(x)\n",
    "print(f\"Match of outputs: {torch.allclose(_out, out)}\")\n",
    "print(f\"Match of hidden layers: {torch.allclose(_hidden, hidden)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f2432-825b-4dde-ba06-3b9e0d325cb9",
   "metadata": {},
   "source": [
    "Let us do one more time step with a new input, this time passing the previously computed hidden values back into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cfe67e-cadf-402b-abf4-d42dcfeb9f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match of outputs: True\n",
      "Match of hidden layers: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, d_in)\n",
    "_out, _hidden = rnn(x, _hidden)\n",
    "out, hidden = forward(x, hidden)\n",
    "print(f\"Match of outputs: {torch.allclose(_out, out)}\")\n",
    "print(f\"Match of hidden layers: {torch.allclose(_hidden, hidden)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699cbb3-a3c4-450e-84b4-5be7b690a8f6",
   "metadata": {},
   "source": [
    "A word about batching. The way how we have extracted the value of the current time step (simply indexing by t) from the input x works as long as the sequence dimension is the first dimension. Therefore, the batch dimension needs to be the second dimension to make this work. Luckily, this is also the way how PyTorch expects the batch dimension for an RNN. Let us repeat our check with batched input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b49bc6-75e5-445b-b534-85375216361d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match of outputs: True\n",
      "Match of hidden layers: True\n"
     ]
    }
   ],
   "source": [
    "B = 4\n",
    "L = 5\n",
    "x = torch.randn(L, B, d_in)\n",
    "_out, _hidden = rnn(x)\n",
    "out, hidden = forward(x)\n",
    "print(f\"Match of outputs: {torch.allclose(_out, out)}\")\n",
    "print(f\"Match of hidden layers: {torch.allclose(_hidden, hidden)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLLM",
   "language": "python",
   "name": "mllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
