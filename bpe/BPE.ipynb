{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ac4241-031d-42d4-802e-64e7d6a95b4c",
   "metadata": {},
   "source": [
    "In this notebook, we explore **byte-pair encoding** described in the paper [Neural machine translation of rare words with subword units](http://arxiv.org/abs/1508.07909v5). The authors also made a [reference implementation](https://github.com/rsennrich/subword-nmt) available on Github which we will use as an additional reference.\n",
    "\n",
    "The idea of BPE is as follows. We start with a vocabulary that initially only contains the basic characters which appear in our text, and tokenize our input according to this vocabulary. We then identify pairs of characters (originally called byte pairs, lending the algorithm its name) which occur frequently in the text next to each other. We introduce a new token for the most frequent pair and re-tokenize the text, taking the new, now larger vocabulary into account. This process called **merge** is repeated iteratively until the vocabulary reaches a given size. In praxis, it will then consist of the most frequent words, a few subwords and still the original set of characters. If we now hit upon an unknown word during inference, we can split it until the individual parts appear in the vocabulary, if needed down to the character level. Therefore unknown token can only appear if new characters show up that we have not seen before, which is easily excluded by fixing a defined character set like ASCII or a subset of the unicode character set.\n",
    "\n",
    "Before going into details, let us quickly discuss one subtle point - word boundaries. In a word-level tokenizer, there is no problem with word boundaries, as each token corresponds to a word. In a subword-level tokenizer, we need to be careful not to forget the information at which points a word ends and the next word starts. In the original paper, a dedicated \"end-of-word\" token \"</w>\" was used for that purpose. This was an ordinary token and thus could get merged with any other token while going through the algorithm. In the current version of the reference implementation, a different approach was chosen - now the end-of-word token is appended to each character while building the initial word list, we will see in a minute how this works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1f2bf-ded3-4e66-a227-821f65481662",
   "metadata": {},
   "source": [
    "Let us now go into the details of the algorithm, using the original paper (that even contained code snippets) and the reference implementation as a guardrail. The first step consists of building up a data structure that we will call the **word frequencies** which is simply a Python dictionary containing all words in the text along with their frequencies (unfortunately this data structure is called the vocabulary in the reference implementation, but we will reserve this term for the set of token that we will identify in the course of the algorithm). Here is a simple function that takes an pre-tokenized example text and creates this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b1e3de-f68a-42ab-a241-08305fdd80a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l o w</w>': 1, 'l o w e r</w>': 1, 'n e w e s t</w>': 1, 'w i d e s t</w>': 1}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def get_word_frequencies(pre_tokenized_text):\n",
    "    counter = collections.Counter(pre_tokenized_text)\n",
    "    word_frequencies = {\" \".join(word) + \"</w>\" : frequency for word, frequency in counter.items() if len(word) > 0}\n",
    "    return word_frequencies\n",
    "\n",
    "pre_tokenized_text = [\"low\", \"lower\",  \"newest\", \"widest\"]\n",
    "word_frequencies = get_word_frequencies(pre_tokenized_text)\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cece8-aa16-4503-a4c2-c8043e2c763a",
   "metadata": {},
   "source": [
    "Note that the input is already pre-tokenized, i.e. split into words, in practice, we could use a PyTorch tokenizer to do this, or any other tokenizer. The reference implementation simply splits along spaces, but the details are not that important. Also note that the keys in our dictionary are not the actual words, but the word as a sequence of characters (which we store as a string to have a hashable key and use spaces as separators, it is therefore important that spaces in the input are removed before processing it, so a space should not appear as a word and no word should contain a space). Also note that, as promised, we append an end-of-word token to the lat character in any word.\n",
    "\n",
    "A second data structure that we need is the actual **vocabulary**, i.e. the set of valid token. Initially, this is simply the set of all characters appearing in any of the words in our input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf304a3-695f-427f-9df7-29c9af7eec64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o', 'r</w>', 's', 'w</w>', 'n', 'i', 'e', 'l', 'd', 'w', 't</w>'}\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(word_frequencies):\n",
    "    vocab = set()\n",
    "    for word in word_frequencies.keys():\n",
    "        for c in word.split():\n",
    "            vocab.add(c)\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocabulary(word_frequencies)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1039182-2d6b-4b1c-8d1e-80f1119e692d",
   "metadata": {},
   "source": [
    "Next, we need to count byte pairs. As we do not cross word boundaries, we can do this on word level, i.e. we go through the words, extract all byte pairs we find there and increase the count of each byte pair by the frequency of the word in the text, thus giving us eventually the frequency of this byte pair (inside word boundaries) in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb48874-593a-41fd-97b2-57a48043aedf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('l', 'o'): 2, ('o', 'w</w>'): 1, ('o', 'w'): 1, ('w', 'e'): 2, ('e', 'r</w>'): 1, ('n', 'e'): 1, ('e', 'w'): 1, ('e', 's'): 2, ('s', 't</w>'): 2, ('w', 'i'): 1, ('i', 'd'): 1, ('d', 'e'): 1})\n"
     ]
    }
   ],
   "source": [
    "def get_stats(word_frequencies):\n",
    "  pairs = collections.defaultdict(int)\n",
    "  for word, freq in word_frequencies.items():\n",
    "    symbols = word.split()\n",
    "    for i in range(len(symbols)-1):\n",
    "      pairs[symbols[i],symbols[i+1]] += freq\n",
    "  return pairs\n",
    "\n",
    "stats = get_stats(word_frequencies)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fb3ed-8353-4fec-ae90-3daa4855dd86",
   "metadata": {},
   "source": [
    "The last basic operation that we need is to conduct a **merge**. First, we identify the pair that occurs most frequently (if more than one pair appears with the same frequency, we follow the convention in the reference implementation to use lexicgraphic ordering next). In our case, this is the pair w, e. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432d6ca1-5c13-4362-b2d2-ff32b6a3b4e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pair: ('w', 'e')\n"
     ]
    }
   ],
   "source": [
    "best_pair = max(stats, key=lambda x: (stats[x], x)) # return tuple in key function, so that comparison of tuples applies\n",
    "print(f\"Best pair: {best_pair}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810cd805-291b-4584-a551-d1ff4e861e4c",
   "metadata": {},
   "source": [
    "We now go through our word frequency dictionary and whenever we encounter the sequence \"w e\", we replace it by \"we\" (at this point our convention to store a sequence of token as a string using spaces as separators pays off, as we can use Python string operations to do this). We also add \"we\" as new token to our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "870b45f4-42de-4383-bea4-091abb8f83ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated word frequencies: {'l o w</w>': 1, 'l o we r</w>': 1, 'n e we s t</w>': 1, 'w i d e s t</w>': 1}\n",
      "Updated vocab: {'o', 'r</w>', 's', 'w</w>', 'n', 'i', 'e', 'l', 'we', 'd', 'w', 't</w>'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def do_merge(best_pair, word_frequencies, vocab):\n",
    "    new_frequencies = dict()\n",
    "    new_token = \"\".join(best_pair)\n",
    "    pair = \" \".join(best_pair)\n",
    "    vocab.add(new_token)\n",
    "    for word, freq in word_frequencies.items():\n",
    "        new_word = re.sub(pair, new_token, word)\n",
    "        new_frequencies[new_word] = word_frequencies[word]\n",
    "    return new_frequencies, vocab\n",
    "\n",
    "\n",
    "word_frequencies, vocab = do_merge(best_pair, word_frequencies, vocab)\n",
    "print(f\"Updated word frequencies: {word_frequencies}\")\n",
    "print(f\"Updated vocab: {vocab}\")\n",
    "rules = []\n",
    "rules.append(best_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029623e-92c9-4e11-93d1-23f5c209ed8e",
   "metadata": {},
   "source": [
    "Unfortunately, our code contains a flaw. Suppose we wanted to merge the symbols \"o\" and \"w\". In the word \"l o w</w>\", the character w appears at the end of the word, i.e. in our encoding scheme, as part of \"w</w>\". This is one symbol which is different from a free-standing \"w\". So we must **not** merge this with the \"o\" preceeding it. Our code, however, would do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41aefa9-93d0-4b91-a549-8af8c7615b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l ow</w>\n"
     ]
    }
   ],
   "source": [
    "best_pair = (\"o\", \"w\")\n",
    "new_token = \"\".join(best_pair)\n",
    "pair = \" \".join(best_pair)\n",
    "word = \"l o w</w>\"\n",
    "new_word = re.sub(pair, new_token, word)\n",
    "print(new_word) # gives a merge which should not be the case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fd967-e255-420f-8edf-c3a7df72ad27",
   "metadata": {},
   "source": [
    "To fix this, we have to use a more sophisticated regular expression that employs a lookahead assertion and a lookbehind assertion (more on this [here](https://docs.python.org/3/library/re.html) to make sure that we only match our best pair if it is surrounded by spaces or word boundaries, i.e. not by regular characters. Also note that we need to escape our byte pair, as it might itself contain characters that have a special meaning inside regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82918aca-2892-4ce3-b551-459e572b082e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l o w</w>\n"
     ]
    }
   ],
   "source": [
    "best_pair = (\"o\", \"w\")\n",
    "new_token = \"\".join(best_pair)\n",
    "pattern = r\"(?<!\\S)\" + re.escape(\" \".join(best_pair)) + r\"(?!\\S)\"\n",
    "word = \"l o w</w>\"\n",
    "new_word = re.sub(pattern, new_token, word)\n",
    "print(new_word) # merge not done with this RE\n",
    "\n",
    "#\n",
    "# Updated merge function\n",
    "#\n",
    "def do_merge(best_pair, word_frequencies, vocab):\n",
    "    new_frequencies = dict()\n",
    "    new_token = \"\".join(best_pair)\n",
    "    pattern = r\"(?<!\\S)\" + re.escape(\" \".join(best_pair)) + r\"(?!\\S)\"\n",
    "    vocab.add(new_token)\n",
    "    for word, freq in word_frequencies.items():\n",
    "        new_word = re.sub(pattern, new_token, word)\n",
    "        new_frequencies[new_word] = word_frequencies[word]\n",
    "    return new_frequencies, vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789755c-bdef-4c68-8eb3-7de0bc2ceb8b",
   "metadata": {},
   "source": [
    "As part of the output of a merge, we also maintain a list of the merges we have done, i.e. the best pairs, as we need to re-apply them later during tokenization. Let us repeat this process for two more times, so that we have done three merges in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5f5a77-b598-4b18-a733-ba3d683e3c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pair: ('s', 't</w>')\n",
      "Best pair: ('l', 'o')\n",
      "Final vocabulary: {'o', 'st</w>', 'r</w>', 's', 'w</w>', 'n', 'i', 'lo', 'e', 'l', 'we', 'd', 'w', 't</w>'}\n",
      "Final words: dict_keys(['lo w</w>', 'lo we r</w>', 'n e we st</w>', 'w i d e st</w>'])\n",
      "Rules: [('w', 'e'), ('s', 't</w>'), ('l', 'o')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    stats = get_stats(word_frequencies)\n",
    "    best_pair = max(stats, key=lambda x: (stats[x], x)) \n",
    "    print(f\"Best pair: {best_pair}\")\n",
    "    word_frequencies, vocab = do_merge(best_pair, word_frequencies, vocab)\n",
    "    rules.append(best_pair)\n",
    "    \n",
    "print(f\"Final vocabulary: {vocab}\")\n",
    "print(f\"Final words: {word_frequencies.keys()}\")\n",
    "print(f\"Rules: {rules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af503dec-3cbd-4ca4-a091-73b2e7e96e85",
   "metadata": {},
   "source": [
    "The output of the algorithm which we will need later on consists of two parts - the vocabulary, i.e. the final set of valid token, and the rules that the model has derived to arrive at the vocabulary, as we will need to apply the same set of rules (in the same order) to a word in order to tokenize it. Before doing this, let us quickly discuss some improvements that are present in the reference implementation but not in our code.\n",
    "\n",
    "* the reference implementation stores the word frequencies as an array where each entry consists of the word and the frequency, not as a dictionary. This makes it easier to modify elements as the words are no longer the keys, and to index individual elements\n",
    "* our code visits all words during a merge, even those words which did not even contain the byte pair that we merge. The reference code maintains an index, which is filled when the pair statistics is calculated and contains essentially a list of all words that contain the respective byte pair. This index is used to only update those words which need to be changed\n",
    "* in addition the statistics are not entirely re-calculated after each merge, but updated incrementally only for those byte pairs that are actually affected by the merge\n",
    "* instead of simply doing a fixed number of merges, the implementation stops if no byte pair can be found whose frequency is above a certain threshold (two by default), so that the algorithm might stop earlier\n",
    "\n",
    "Let us now discuss how tokenization can be implemented once we have determined the vocabulary and the rules. The idea is again described in the original paper - given an unknown word, we simply apply the rules in the order in which we have derived and recorded them to the new word. Of course, there are some shortcuts that we can take, for instance by maintaining a cache of words that we have already seen, but let us again focus on the straightforward implementation in this notebook. This is actually rather simple - to encode a word we first turn it into the same format as during training, i.e. into a chain of characters, separated by whitespaces and followed by an end-of-word marker, and then use regular expressions as during training to run the replacements per rule. We will also have to build a lookup table mapping items in the vocabulary to indices so that the encoding eventually returns a list of IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce53fe9-60a2-4a95-8b43-0ec0d5683671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stoi = dict()\n",
    "for idx, symbol in enumerate(vocab):\n",
    "    stoi[symbol] = idx\n",
    "\n",
    "def encode(word):\n",
    "    _word = \" \".join(word) + \"</w>\"\n",
    "    #\n",
    "    # apply rules in the original order\n",
    "    #\n",
    "    for r, bp in enumerate(rules):\n",
    "        new_token = \"\".join(bp)\n",
    "        pattern = re.compile(r'(?<!\\S)' + re.escape(\" \".join(bp)) + r'(?!\\S)')\n",
    "        _word = pattern.sub(new_token, _word)\n",
    "    indices = [stoi[symbol] for symbol in _word.split()]\n",
    "    return _word, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f952821d-f1d3-4676-8b44-1160fb26a3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lo w</w>', [7, 4])\n",
      "('n e we st</w>', [5, 8, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"low\"))\n",
    "print(encode(\"newest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19bac9-8da4-4fd8-b810-70d6628477e5",
   "metadata": {},
   "source": [
    "Of course there is still a way to go to turn this into a real implementation. One obvious point is that we do not yet handle unknown symbols, so we should add a second meta-symbol to our vocabulary when starting the learning phase that represents an unknown symbol. During the lookup at the end of the encoding, we would then turn any failed lookup into a default index. There are also some shortcuts we can take during encoding that speed up the process substantially:\n",
    "* implement a cache\n",
    "* if the word appears as-is in the vocabulary, return the corresponding symbol immediately\n",
    "* if we process the rules and arrive at a point where the remaining word does not contain any additional spaces (i.e. consists of one final symbol), exit the loop as we are done\n",
    "* compile all regular expressions upfront and store them along with the rules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
