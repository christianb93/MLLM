{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d17eb8-7ad4-4b1f-964c-022b3f4e05b0",
   "metadata": {},
   "source": [
    "In this notebook, we will take a closer look at how attention is implemented in PyTorch. For that purpose, we will first do a naive implementation of multi-head attention, using the formula \n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{MultiHeadAttention}(Q, K, V) = (\\textrm{head}_1, \\dots, \\textrm{head}_h) W^O\n",
    "\\end{align*}\n",
    "\n",
    "where each head is given by\n",
    "\n",
    "$$\n",
    "\\textrm{head}_i = \\textrm{softmax}\\left(\\frac{Q W_i^Q (K W^K_i)^t}{\\sqrt{d_k}}\\right) (V W_i^V) \\, i = 0, \\dots, h \n",
    "$$\n",
    "\n",
    "as the [original transformer paper](https://arxiv.org/abs/1706.03762) specifies it. It is straightforward to implement this as a module using PyTorch. Once we have this, we will learn how to extract the weight matrices that appear in this formula from the [PyTorch multi-head attention layer](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) and will verify our understanding by running our implementation and the PyTorch module on the same input to see that they yield the same results (on non-batched input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20906c93-9672-455f-a555-58f9592f9803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2d7de8-a3e9-4343-ab7a-8aaf17082d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D, kdim = None, vdim = None, heads = 1):\n",
    "        super().__init__()\n",
    "        self._D = D\n",
    "        self._heads = heads\n",
    "        self._kdim = kdim if kdim is not None else D // heads\n",
    "        self._vdim = vdim if vdim is not None else D // heads\n",
    "        for h in range(self._heads):\n",
    "            wq_name = f\"_wq_h{h}\"\n",
    "            wk_name = f\"_wk_h{h}\"\n",
    "            wv_name = f\"_wv_h{h}\"\n",
    "            wq = torch.randn(self._D, self._kdim)\n",
    "            wk = torch.randn(self._D, self._kdim)\n",
    "            wv = torch.randn(self._D, self._vdim)\n",
    "            setattr(self, wq_name, torch.nn.Parameter(wq))\n",
    "            setattr(self, wk_name, torch.nn.Parameter(wk))\n",
    "            setattr(self, wv_name, torch.nn.Parameter(wv))\n",
    "        wo = torch.randn(self._heads*self._vdim, self._D)\n",
    "        self._wo = torch.nn.Parameter(wo)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        for h in range(self._heads):\n",
    "            wq_name = f\"_wq_h{h}\"\n",
    "            wk_name = f\"_wk_h{h}\"\n",
    "            wv_name = f\"_wv_h{h}\"\n",
    "            Q = X@getattr(self, wq_name)\n",
    "            K = X@getattr(self, wk_name)\n",
    "            V = X@getattr(self, wv_name)\n",
    "            head = Q@K.t() / math.sqrt(float(self._kdim))\n",
    "            head = torch.softmax(head, dim = -1)\n",
    "            head = head@V\n",
    "            if 0 == h:\n",
    "                out = head\n",
    "            else:\n",
    "                out = torch.cat([out, head], dim = 1)\n",
    "        return out@self._wo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af8b98-a47c-4d6b-ac91-28b1df60d330",
   "metadata": {},
   "source": [
    "Let us now try to match this to the PyTorch implementation. We start with the case of only one head. Let us create an input, a PyTorch model and our own model and try to synchronize the parameters. Unfortunately PyTorch does not provide an easy way to access the weights, but a short look at the source code provides the following translation table (which, however, only holds for the case that all dimensions are equal):\n",
    "\n",
    "| Attribute in torch.nn.MultiheadAttention | Weight matrix |\n",
    "| ---                                      | ---           |\n",
    "| out_proj                                 | $W^O$         |\n",
    "| in_proj_weight                           | $W^Q$, $W^K$ and $W^V$|\n",
    "\n",
    "More specifically, [this function](https://github.com/pytorch/pytorch/blob/5ee5a164ffeb7b7a167c53009fb8fe5f5bd439d9/torch/nn/functional.py#L4732) suggest that the the weight matrices are placed in one tensor *in_proj_weight* packed along dimension 0, in q, k, v order. Also note that the out projection is an instance of *torch.nn.Linear*, and [this function](https://github.com/pytorch/pytorch/blob/5ee5a164ffeb7b7a167c53009fb8fe5f5bd439d9/torch/nn/functional.py#L4732) reveals that PyTorch uses *torch.nn.linear* under the hood instead of a plain matrix multiplication to carry out the projections, which uses the **transpose** of the weight matrix. Thus we have to transpose the weight matrices from the PyTorch model before pulling them into our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6b3103-c8fc-4b74-986f-5a4d22bb58e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of wq: torch.Size([3, 3])\n",
      "Shape of wk: torch.Size([3, 3])\n",
      "Shape of wv: torch.Size([3, 3])\n",
      "Shape of wo: torch.Size([3, 3])\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "D = 3\n",
    "L = 2\n",
    "ptAttention = torch.nn.MultiheadAttention(embed_dim = D, num_heads = 1)\n",
    "myAttention = MultiHeadSelfAttention(D, heads = 1)\n",
    "#\n",
    "# Extract weights \n",
    "#\n",
    "wq, wk, wv = ptAttention.in_proj_weight.chunk(3)\n",
    "wo = ptAttention.out_proj\n",
    "print(f\"Shape of wq: {wq.shape}\")\n",
    "print(f\"Shape of wk: {wk.shape}\")\n",
    "print(f\"Shape of wv: {wv.shape}\")\n",
    "wo = wo.weight\n",
    "print(f\"Shape of wo: {wo.shape}\")\n",
    "#\n",
    "# Copy weights to our model\n",
    "#\n",
    "myAttention._wq_h0 = torch.nn.Parameter(wq.clone().t())\n",
    "myAttention._wk_h0 = torch.nn.Parameter(wk.clone().t())\n",
    "myAttention._wv_h0 = torch.nn.Parameter(wv.clone().t())\n",
    "myAttention._wo = torch.nn.Parameter(wo.clone().t())\n",
    "#\n",
    "# Create input, feed through both models and compare\n",
    "#\n",
    "X = torch.randn(L, D)\n",
    "out, _ = ptAttention(X, X, X)\n",
    "_out = myAttention(X)\n",
    "print(f\"Outputs match: {torch.allclose(out, _out)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5c413-238f-4680-94ca-41f0b884d858",
   "metadata": {},
   "source": [
    "let us now go in more detail through the PyTorch implementation of attention which is essentially [this function](https://github.com/pytorch/pytorch/blob/37cde56658e20afae6d94b70d53e4131043e09e8/torch/nn/functional.py#L5025) and repeat what it does, ignoring a few special cases.\n",
    "\n",
    "Our example will use low dimensions to be able to track the inputs visually. Specifically, we will use two heads, embedding dimension 4 (which, as it should be, is a multiple of the number of heads) and sequence length $L = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a04455e-1c0a-4c9c-9157-5f8aea58984c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L = 2\n",
    "D = 4\n",
    "h = 2\n",
    "#\n",
    "# Prepare an input X of shape L x D, i.e. with 8 elements\n",
    "#\n",
    "X = torch.arange(1, 9).reshape(L, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c9fae-584d-4b46-b0e8-14b976a6d94e",
   "metadata": {},
   "source": [
    "For every head $i$, we have three weight matrices $W_i^Q$, $W_i^V$ and $W_i^K$. The key and value dimension as well as the query dimension are two (embedding dimension divided by number of heads). Each of these matrices is of dimension $D \\times d_k$ and therefore has $d_k D$ parameters. The total number of parameters is therefore $3 h d_k D = 3 D^2$, so that we could as well organize our weights in a single matrix $W$ of dimensions $3 D \\times D$. Let us locate this matrix in the PyTorch attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b72eef-fec2-4ca0-89d0-fbf696373b96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4])\n"
     ]
    }
   ],
   "source": [
    "ptAttention = torch.nn.MultiheadAttention(embed_dim = D, num_heads = h)\n",
    "W = ptAttention.in_proj_weight.clone()\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e8b99-18ba-4572-8a5d-442421d80d9d",
   "metadata": {},
   "source": [
    "Let us now follow the path of our input through the attention layer. We start with a new value of $W$ and $X$ to be able to identify the individual components of $W$ more easily as they are propagated through the code. If we pass our input and $W$ to the forward function of the attention module, PyTorch will first [unsqueeze our input](https://github.com/pytorch/pytorch/blob/37cde56658e20afae6d94b70d53e4131043e09e8/torch/nn/functional.py#L5153) to put it into the form $L \\times B \\times D$, where $B = 1$ is the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f12953-a78a-4622-9375-37f4c0a9dcb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 4])\n",
      "torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "W = torch.arange(1, 1+D*D*3).reshape(3*D, D).to(torch.float32)\n",
    "print(W.shape)\n",
    "X = torch.arange(51, 59).reshape(L, D).to(torch.float32)\n",
    "X = X.unsqueeze(1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37da20-a81b-4b9d-bf5d-c83a7809a34c",
   "metadata": {},
   "source": [
    "Next a few parameters are calculated, namely the head dimension (which is the embedding dimension divided by the number of heads) and the target and source length (which is 𝐿\n",
    "in our case). So the head dimension is 2 and the target and source length are both equal to 𝐿=2. In the next step, the projections onto query, key and value are calculated in [this function](https://github.com/pytorch/pytorch/blob/37cde56658e20afae6d94b70d53e4131043e09e8/torch/nn/functional.py#L4732). In the most general case, where the inputs are not equal, this will split 𝑊 into three parts of dimension 𝐷×𝐷 and apply torch.nn.functional.linear to query, key and value using the resulting matrices as weights. Note that this will calculate the product of the input with the transpose of the weight matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84fbad42-956c-4a86-8b31-6515ed9d1464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  5.,  9., 13.],\n",
      "        [ 2.,  6., 10., 14.],\n",
      "        [ 3.,  7., 11., 15.],\n",
      "        [ 4.,  8., 12., 16.]])\n",
      "tensor([[17., 21., 25., 29.],\n",
      "        [18., 22., 26., 30.],\n",
      "        [19., 23., 27., 31.],\n",
      "        [20., 24., 28., 32.]])\n",
      "tensor([[ 1.,  5.,  9., 13.],\n",
      "        [ 2.,  6., 10., 14.],\n",
      "        [ 3.,  7., 11., 15.],\n",
      "        [ 4.,  8., 12., 16.]])\n"
     ]
    }
   ],
   "source": [
    "_w_q, _w_k, _w_v = W.chunk(3)\n",
    "w_q = _w_q.t()\n",
    "w_k = _w_k.t()\n",
    "w_v = _w_v.t()\n",
    "print(w_q)\n",
    "print(w_k)\n",
    "print(w_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1043414-b51b-467a-a5de-2e6b2a694a41",
   "metadata": {},
   "source": [
    "Next, query, key and value are determined by applying a linear layer with the respective weights to the input. Note that in PyTorch, applying a linear layer without bias amounts to multiplying the input matrix from the right with the **transpose** of the weight matrix, this is why we have applied the transpose when extracting the weight matrices above. Let us go through this and verify that the linear layer gives in fact the same result as the product with the transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4afedf-9f09-4a3b-8518-c6fc4b0eadba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 530., 1370., 2210., 3050.]],\n",
      "\n",
      "        [[ 570., 1474., 2378., 3282.]]])\n"
     ]
    }
   ],
   "source": [
    "_q = torch.nn.functional.linear(X, _w_q)\n",
    "q = torch.matmul(X, w_q)\n",
    "print(q)\n",
    "assert(torch.allclose(q, _q))\n",
    "k = torch.matmul(X, w_k)\n",
    "v = torch.matmul(X, w_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77be093-48de-42c5-a33e-77e962e73d59",
   "metadata": {},
   "source": [
    "Note that query, key and value for each head are matrices of dimensions $L \\times B \\times 2$, and this is a matrix of dimension $L \\times B \\times D$, so it still contains the information for all heads. \n",
    "\n",
    "[Back in the main function](https://github.com/pytorch/pytorch/blob/37cde56658e20afae6d94b70d53e4131043e09e8/torch/nn/functional.py#L5247), the three matrices are reshaped. First, the matrix is reshaped to dimension $(L, B \\cdot h, D / h)$, i.e. each columm of $W$ is split into one part for every head. Then, the first and second dimension of the resulting matrix are switched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02f5791-19cf-403a-b3fe-598d760fa28c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 530., 1370.],\n",
      "         [ 570., 1474.]],\n",
      "\n",
      "        [[2210., 3050.],\n",
      "         [2378., 3282.]]])\n"
     ]
    }
   ],
   "source": [
    "B = 1\n",
    "head_dim = D // h\n",
    "q = q.view(L, B * h, head_dim).transpose(0, 1)\n",
    "k = k.view(L, B * h, head_dim).transpose(0, 1)\n",
    "v = v.view(L, B * h, head_dim).transpose(0, 1)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb0fc0-8521-4596-bea6-45e2487e5562",
   "metadata": {},
   "source": [
    "So now the first dimension is reflecting both the batch dimensions and the different heads. As our batch size is one, we can easily print the values of $q$ for both heads. Note that this arrangement will allow us to apply batched multiplication, i.e. we essentially treat the head dimension as an additional batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c700dc9b-b4df-46da-91e5-8857e78f4e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 530., 1370.],\n",
      "        [ 570., 1474.]])\n",
      "tensor([[2210., 3050.],\n",
      "        [2378., 3282.]])\n"
     ]
    }
   ],
   "source": [
    "q_0 = q[0, :, :]\n",
    "q_1 = q[1, :, :]\n",
    "print(q_0)\n",
    "print(q_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d823e-9a38-4982-b5e3-b13fea863764",
   "metadata": {},
   "source": [
    "Let us now try to understand by which matrix we would have to multiply $X$ to get the same result. The values that we see in the query for head 0 are the elements of the first two columns of $q = X \\cdot w_q$, ignoring the batch dimension. We are looking for a matrix of dimension $4 \\times 2$ that, when being multiplied by $X$, gives $q_0$. This matrix should be of dimension $D \\times 2$, i.e. embedding dimension times head dimension. Let us try to split $w_q$ along the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6894805d-3045-46f7-af0b-57e02ad52b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 5.],\n",
      "        [2., 6.],\n",
      "        [3., 7.],\n",
      "        [4., 8.]])\n",
      "tensor([[ 530., 1370.],\n",
      "        [ 570., 1474.]])\n"
     ]
    }
   ],
   "source": [
    "w_q_0, w_q_1 = w_q.split(head_dim, dim = 1)\n",
    "print(w_q_0)\n",
    "_q_0 = torch.matmul(X, w_q_0)[:, 0, :]\n",
    "_q_1 = torch.matmul(X, w_q_1)[:, 0, :]\n",
    "print(_q_0)\n",
    "assert(torch.allclose(q_0, _q_0))\n",
    "assert(torch.allclose(q_1, _q_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873d1c6-7e11-4249-87ed-eb2f082d25a1",
   "metadata": {},
   "source": [
    "Next, the attention scores are determined [here](https://github.com/pytorch/pytorch/blob/37cde56658e20afae6d94b70d53e4131043e09e8/torch/nn/functional.py#L5307), using batched multiplication (let us suppose that we have requested those, so that we enter the corresponding branch in the function). For that purpose, the function *torch.bmm* is invoked, which performs a batch multiplication (and assumes that the batch dimension is the first one, which was the reason for the reordering). This gives the attention matrix per batch item and head. Then softmax is applied to the last dimension. Finally, the attention weights are multiplied by $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a50081-0f4b-48d0-a2b3-9ffdda42c7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "A = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(float(head_dim))\n",
    "A = torch.softmax(A, dim = -1)\n",
    "out = torch.bmm(A, v)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee9033-d63d-4fcd-a76a-631e123bdc6b",
   "metadata": {},
   "source": [
    "As expected, the output has the dimensions $(B*h, L, L)$. Next, the batch / head dimension is moved back into the middle, and the first two dimensions are combined into one. Finally, the output projection is applied to this batch again and the result is returned.\n",
    "\n",
    "Armed with this understanding, let us now try to set up a PyTorch attention layer with two heads, extract the weights, feed them into our implementation and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4322505-d6a8-490c-ae8f-1b26efddd6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "Outputs equal: True\n"
     ]
    }
   ],
   "source": [
    "D = 4\n",
    "L =2\n",
    "h = 2\n",
    "head_dim = D // h\n",
    "ptAttention = torch.nn.MultiheadAttention(embed_dim = D, num_heads = h)\n",
    "myAttention = MultiHeadSelfAttention(D, heads = h)\n",
    "#\n",
    "# Extract weights from W as before\n",
    "#\n",
    "_w_q, _w_k, _w_v = ptAttention.in_proj_weight.chunk(3)\n",
    "w_q = _w_q.clone().t()\n",
    "w_k = _w_k.clone().t()\n",
    "w_v = _w_v.clone().t()\n",
    "w_q_0, w_q_1 = w_q.split(head_dim, dim = 1)\n",
    "w_k_0, w_k_1 = w_k.split(head_dim, dim = 1)\n",
    "w_v_0, w_v_1 = w_v.split(head_dim, dim = 1)\n",
    "#\n",
    "# Inject weights into our model\n",
    "#\n",
    "myAttention._wq_h0 = torch.nn.Parameter(w_q_0)\n",
    "myAttention._wq_h1 = torch.nn.Parameter(w_q_1)\n",
    "myAttention._wk_h0 = torch.nn.Parameter(w_k_0)\n",
    "myAttention._wk_h1 = torch.nn.Parameter(w_k_1)\n",
    "myAttention._wv_h0 = torch.nn.Parameter(w_v_0)\n",
    "myAttention._wv_h1 = torch.nn.Parameter(w_v_1)\n",
    "wo = ptAttention.out_proj.weight.clone().t()\n",
    "myAttention._wo = torch.nn.Parameter(wo)\n",
    "#\n",
    "# Generate input, feed into both models and compare\n",
    "#\n",
    "X = torch.randn(L, D)\n",
    "print(X.shape)\n",
    "out = myAttention(X)\n",
    "_out, _ = ptAttention(X, X, X)\n",
    "print(f\"Outputs equal: {torch.allclose(_out, out)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLLM",
   "language": "python",
   "name": "mllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
