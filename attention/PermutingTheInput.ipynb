{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6120f7-5559-487b-886e-b377a7d3004d",
   "metadata": {},
   "source": [
    "In this notebook, we will investigate how a transformer layer reacts on permuted input, i.e. changing the order of words in a sentence. We keep the model dimensions small to be able to visually inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da08339b-f9e6-4bb1-ad9a-01d95da4b704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9832302930>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb2be7a-7d68-46f1-9b24-a94beb148a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model dimension\n",
    "D = 8\n",
    "# Length of encoder input\n",
    "L = 4\n",
    "#\n",
    "# Create random input\n",
    "#\n",
    "X = torch.randn(L, D)\n",
    "#\n",
    "# and feed through an encoder block\n",
    "#\n",
    "block = torch.nn.TransformerEncoderLayer(d_model = D, nhead = 4, dropout = 0)\n",
    "Y = block(X).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dac3c9-f20c-4bb8-a534-8ef4a37466f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Now permute the input, recompute\n",
    "#\n",
    "Xp = X.flip(dims = [0]).detach()\n",
    "Yp = block(Xp).detach()\n",
    "#\n",
    "# Verify that Yp is simply the permutation of Y\n",
    "#\n",
    "print(f\"{torch.allclose(Yp, Y.flip(dims = [0]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2cad35-75e5-4822-810f-ed83db98fe3d",
   "metadata": {},
   "source": [
    "So we have confirmed that permuting the input of a decoder layer simply results in the same permutation being applied to the output. Now let us simulate that this encoder output is fed as input into the attention layer of a decoder. Thus the keys and values are the encoder output Y respectively Yp, while the queries come from the decoder input and are unpermuted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c911aa7a-08a2-4826-88f5-4249c8a16507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# length of target sequence, i.e. decoder input\n",
    "T = 3 \n",
    "queries = torch.randn(T, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "147ba746-d5bd-4d06-812c-32ccb935227c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = torch.nn.MultiheadAttention(embed_dim = D, num_heads = 4)\n",
    "#\n",
    "# Put into eval mode to avoid dropout\n",
    "#\n",
    "attn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4205df-56ca-4bb4-88de-b46a09b2a5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Values and keys are both the encoder output\n",
    "#\n",
    "out = attn(queries, Y, Y)[0].detach()\n",
    "outp = attn(queries, Yp, Yp)[0].detach()\n",
    "#\n",
    "# Compare\n",
    "#\n",
    "print(f\"{torch.allclose(out, outp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9991a2-7803-4d8f-8bca-016ef85c6936",
   "metadata": {},
   "source": [
    "Thus the attention layer sitting between the encoder and the decoder layers is invariant under permutations, i.e. permuting the input does not change the output. Taking these two observations together implies that when permuting the inputs to an encoder-decoder combination,  i.e. the source sentence, the output of the model does not change. Thus the model is insensitive towards permutations. We can also verify this with a full transformer directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fd4ecf0-503d-483a-9c67-a13e33fa4aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3313,  0.3166, -1.6677, -0.7317,  0.4747, -1.0081,  0.0325,  1.2522],\n",
      "        [ 1.2092,  0.2136, -2.0010,  0.2608,  0.4414, -0.7903, -0.4971,  1.1634],\n",
      "        [ 1.0899, -0.1775, -2.1671, -0.3654,  0.9013, -0.4634,  0.2556,  0.9265]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.3313,  0.3166, -1.6677, -0.7317,  0.4747, -1.0081,  0.0325,  1.2522],\n",
      "        [ 1.2092,  0.2136, -2.0010,  0.2608,  0.4414, -0.7903, -0.4971,  1.1634],\n",
      "        [ 1.0899, -0.1775, -2.1671, -0.3654,  0.9013, -0.4634,  0.2556,  0.9265]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "transformer = torch.nn.Transformer(d_model = D, nhead = 1)\n",
    "transformer.eval()\n",
    "tgt = torch.randn(T, D)\n",
    "src = torch.randn(L, D)\n",
    "src_permuted = src.flip(dims = [0])\n",
    "out = transformer(src, tgt)\n",
    "_out = transformer(src_permuted, tgt)\n",
    "print(out)\n",
    "print(_out)\n",
    "print(f\"{torch.allclose(out, _out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380385c-ab13-43ee-afd0-a214105a5406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLLM",
   "language": "python",
   "name": "mllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
