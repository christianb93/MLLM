{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119f182c-7312-4440-889a-59ef2d752b5c",
   "metadata": {
    "id": "119f182c-7312-4440-889a-59ef2d752b5c"
   },
   "source": [
    "As we would expect from an encoder-decoder architecture, T5 is mainly a sequence-to-sequence or text-to-text model, i.e. its input is a sequence of token, and its output is a different sequence of token which, in general, does not have the same length as the input. In the training method used for T5, the input and labels for different tasks are all converted into pairs of sentences, so that the same model can be applied to a large variety of tasks. The firsts part of the sentence is fed into the encoder, the second part - the labels - are fed as targets into the decoder which is then trained using teacher-forcing on these labels.\n",
    "\n",
    "Let us first load a pretrained T5 model and try an example, specifically we reproduce the example for a single training step given in the [Huggingface documentation](https://huggingface.co/docs/transformers/model_doc/t5). This reflects the training method that was used for unsupervised pretraining that is called **span corruption**. With this method, the model receives an input in which spans of words are replaced by a special token. The model is then trained to predict the value of these masked spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "PJLEvRb1gE_C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJLEvRb1gE_C",
    "outputId": "9a8aaa21-dc39-4134-9448-d018d27cda76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.27.* in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (4.27.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (2023.6.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (1.24.2)\n",
      "Requirement already satisfied: filelock in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (3.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (6.0)\n",
      "Requirement already satisfied: requests in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (4.65.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from transformers==4.27.*) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.*) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.*) (2023.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from requests->transformers==4.27.*) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from requests->transformers==4.27.*) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from requests->transformers==4.27.*) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (from requests->transformers==4.27.*) (1.26.15)\n",
      "Requirement already satisfied: sentencepiece==0.1.97 in /home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages (0.1.97)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.27.*\n",
    "!pip3 install sentencepiece==0.1.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7836ff2d-6b2a-4910-b5f9-27d005a9ce05",
   "metadata": {
    "id": "7836ff2d-6b2a-4910-b5f9-27d005a9ce05",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Import libraries and load model\n",
    "#\n",
    "import transformers\n",
    "import torch\n",
    "model_version  = \"t5-small\"\n",
    "tokenizer = transformers.T5Tokenizer.from_pretrained(model_version, model_max_length=512)\n",
    "model = transformers.T5ForConditionalGeneration.from_pretrained(model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e974e1ab-558b-4f36-89d6-efd6809cc831",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e974e1ab-558b-4f36-89d6-efd6809cc831",
    "outputId": "54a27b8f-52ec-4774-88a5-88de5ccfd90a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'])\n",
      "3.7837319374084473\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Next we will prepare the encoder input and the decoder input. The encoder input is\n",
    "# the masked sentence\n",
    "#\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\").input_ids\n",
    "input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(dim=0)\n",
    "#\n",
    "# The labels (the target used to calculate the loss function) consists of the true values for each mask\n",
    "# extra_id_0 ---> dog\n",
    "# extra_id_1 ---> the\n",
    "#\n",
    "labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\").input_ids\n",
    "labels = torch.tensor(labels, dtype=torch.long).unsqueeze(dim=0)\n",
    "#\n",
    "# Run that through model. We specify the decoder inputs using the labels argument\n",
    "#\n",
    "out = model(input_ids= input_ids, labels = labels)\n",
    "print(out.__dict__.keys())\n",
    "print(out.loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ywizpobR-nSh",
   "metadata": {
    "id": "ywizpobR-nSh"
   },
   "source": [
    "Let us understand what is going on. First, we create our input, containing a masked version of the sentence \"The cute dog walks in the park\", with the two spans \"cute dog\" and \"the\" replaced by special token, so called **sentinel token**. Then we create the corresponding target, which consists of each sentinel token, followed by the expected value of the token. We then call the [forward method](https://github.com/huggingface/transformers/blob/f2cc8ffdaaad4ba43343aab38459c4208d265267/src/transformers/models/t5/modeling_t5.py#L1617) of the model.\n",
    "\n",
    "The forward method first sends the input_ids through the encoder to obtain its representation in the internal model dimension, called encoder_outputs. It then takes the labels and shifts them by one position to the right, by applying the method `_shift_right`. More precisely, we shift the labels by one position to the right, ignoring the last token but filling up with a special token called the decoder start token on the right. Note that the token that we loose in this way is the end-of-sentence token, while the token that we use fill up is at the same time the decoder start token and the pad token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc11882-9ad5-4f01-b493-e711ec1e3088",
   "metadata": {
    "id": "cdc11882-9ad5-4f01-b493-e711ec1e3088"
   },
   "source": [
    "During inference, we do not pass the labels but the input ids and the decoder input ids. Let us try this out. We use the same input ids as before. As decoder input, we use a sequence consisting only of the decoder start token. We then retrieve the logits from the model and take the argmax, corresponding to greedy search. Note that we need to add an extra batch dimension as dimension zero for both inputs as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce2550c-6004-47b8-a518-1c630bc945a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce2550c-6004-47b8-a518-1c630bc945a9",
    "outputId": "1ba13b63-942b-4fbd-c7de-3a9e5873129c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n",
      "torch.Size([1, 1, 32128])\n",
      "<extra_id_0>\n"
     ]
    }
   ],
   "source": [
    "decoder_input_ids = torch.tensor([model.config.decoder_start_token_id], dtype=torch.long).unsqueeze(dim = 0)\n",
    "print(decoder_input_ids)\n",
    "out = model(input_ids = input_ids, decoder_input_ids = decoder_input_ids)\n",
    "logits = out.logits\n",
    "print(logits.shape) # (B, L, V)\n",
    "sample_idx = torch.argmax(logits[0, -1, :]).item()\n",
    "print(tokenizer.convert_ids_to_tokens(sample_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124aa50-662f-43c1-88f1-f77fc6f09945",
   "metadata": {
    "id": "e124aa50-662f-43c1-88f1-f77fc6f09945"
   },
   "source": [
    "This is actually what we expect, as we want the output to be in the same form as before. Let us now proceed like this - we concatenate our output to the decoder input and run through the same procedure again. We repeat this process until we have sampled a given maximum number of token or reach an end-of-sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cee5cb8-6144-46db-8a58-c911784eacdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cee5cb8-6144-46db-8a58-c911784eacdf",
    "outputId": "66006960-5e01-4877-daea-9eb8334fa304",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2>park.\n"
     ]
    }
   ],
   "source": [
    "def sample_from_input_string(input_string, model, tokenizer):\n",
    "    input_ids = tokenizer(input_string).input_ids\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(dim = 0)\n",
    "    #\n",
    "    # first iteration\n",
    "    #\n",
    "    decoder_input_ids = torch.tensor([model.config.decoder_start_token_id], dtype=torch.long).unsqueeze(dim = 0)\n",
    "    logits = model(input_ids = input_ids, decoder_input_ids = decoder_input_ids).logits\n",
    "    sample_idx = torch.argmax(logits[0, -1, :]).item()\n",
    "    #\n",
    "    # additional iterations\n",
    "    #\n",
    "    for i in range(10):\n",
    "        sample = torch.tensor([sample_idx], dtype=torch.long).unsqueeze(dim=0)\n",
    "        decoder_input_ids = torch.cat((decoder_input_ids, sample), dim = 1)\n",
    "        logits = model(input_ids = input_ids, decoder_input_ids = decoder_input_ids).logits\n",
    "        sample_idx = torch.argmax(logits[0, -1, :]).item()\n",
    "        if sample_idx == model.config.eos_token_id:\n",
    "            break\n",
    "    #\n",
    "    # Decode\n",
    "    #\n",
    "    outputs = decoder_input_ids[0, :].tolist()\n",
    "    return tokenizer.decode(outputs)\n",
    "\n",
    "print(sample_from_input_string(\"The <extra_id_0> walks in <extra_id_1> park\", model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5dfd33-fcea-41f5-a296-5611b15e7f7b",
   "metadata": {
    "id": "be5dfd33-fcea-41f5-a296-5611b15e7f7b"
   },
   "source": [
    "This is the same result that we also find in the Huggingface documentation, note that the outcome is deterministic, as we use greedy search here. So we have successfully reproduced the generate method. However, this is still fairly inefficient. In the forward method of the transformer, we first feed the input ids into the encoder, however, these input ids have the same value for each iteration! Fortunately, the model returns the hidden state of the last layer of the encoder (a tensor of shape $B \\times L \\times D$) along with the outputs, and allows us to feed this as an additional input into subsequent calls of the forward method. Here is a modified sampling method using this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d597c17b-96c0-40f8-bb1b-2cb4eac3d44a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d597c17b-96c0-40f8-bb1b-2cb4eac3d44a",
    "outputId": "35dd77b7-6939-479b-bfe0-28f50aad9030",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0> park offers<extra_id_1> the<extra_id_2>park.\n"
     ]
    }
   ],
   "source": [
    "def sample_from_input_string(input_string, model, tokenizer, remove_padding = False):\n",
    "    t = tokenizer(input_string)\n",
    "    input_ids = t.input_ids\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(dim = 0)\n",
    "    #\n",
    "    # first iteration\n",
    "    #\n",
    "    decoder_input_ids = torch.tensor([model.config.decoder_start_token_id], dtype=torch.long).unsqueeze(dim = 0)\n",
    "    out = model(input_ids = input_ids, decoder_input_ids = decoder_input_ids)\n",
    "    logits = out.logits\n",
    "    hidden_state = out.encoder_last_hidden_state\n",
    "    sample_idx = torch.argmax(logits[0, -1, :]).item()\n",
    "    #\n",
    "    # additional iterations\n",
    "    #\n",
    "    for i in range(50):\n",
    "        sample = torch.tensor([sample_idx], dtype=torch.long).unsqueeze(dim=0)\n",
    "        decoder_input_ids = torch.cat((decoder_input_ids, sample), dim = 1)\n",
    "        logits = model(input_ids = input_ids, decoder_input_ids = decoder_input_ids, encoder_outputs = [hidden_state]).logits\n",
    "        sample_idx = torch.argmax(logits[0, -1, :]).item()\n",
    "        if sample_idx == model.config.eos_token_id:\n",
    "            break\n",
    "    #\n",
    "    # Decode result (and remove padding if requested)\n",
    "    #\n",
    "    if remove_padding:\n",
    "        outputs = decoder_input_ids[0, 1:].tolist()\n",
    "    else:\n",
    "        outputs = decoder_input_ids[0, :].tolist()\n",
    "    return tokenizer.decode(outputs)\n",
    "\n",
    "print(sample_from_input_string(\"The <extra_id_0> walks in <extra_id_1> park\", model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae3627-4f4e-4a29-b4cf-9ac757595a4c",
   "metadata": {
    "id": "48ae3627-4f4e-4a29-b4cf-9ac757595a4c"
   },
   "source": [
    "The examples that we have used so far corresponding to the first training phase that was applied to the model - the unsupervised pre-training using span corruption. In addition, the model did undergo supervised trainings on specific tasks. Let us take a closer look at some of them.\n",
    "\n",
    "The general idea of how to train T5 on these specific tasks is to add a task-specific prefix to the input which tells the model which task it is supposed to carry out. These prefixes are described in the appendix of the original paper - for translation, the prefix is simply \"translate English to German\" (or German replaced by the language of your choice). Apart from that, the processing pattern is exactly as before, so we can simply reuse the function that we have already put together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378b9681-46ec-4ccc-a2fc-469ef79ac23e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "378b9681-46ec-4ccc-a2fc-469ef79ac23e",
    "outputId": "9ec8a0c0-a5c1-4718-b005-fb5e99eabbfc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Haus ist wunderbar.\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"translate English to German: The house is wonderful.\", model, tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21230d1d-1194-45b9-9cc5-bd54793882f0",
   "metadata": {
    "id": "21230d1d-1194-45b9-9cc5-bd54793882f0"
   },
   "source": [
    "This is nice - exactly the same model with the same inference (and training) methods can be used for translation as well. This is the general idea behind models like T5 - instead of fine-tuning via transfer learning which involves putting together a task-specific model that takes over some of the layers of the pre-trained model, we train the exactly same model using essentially the same code.\n",
    "\n",
    "Translation is not the only downstream task on which T5 has been trained. As an example for a different task, let us look at natural language inference. T5 has been training on the MNLI corpus described [in this paper](https://arxiv.org/abs/1704.05426). The general structure of the task is as follows. The model is given two sentences, where the first one is a premise and the second one is a hypothesis. The task is to predict whether the hypothesis follows from the premise (\"entailment\"), is contradicting the premise (\"contradiction\") or neither nor applies (\"neutral\"). Again, the task is specified by using a task specific prefix, namely \"mnli\", followed by \"premise\", followed by the premise, and then \"hypothesis\" before finally the hypothesis is added to the input. Here are a few examples, where the first one is taken from the T5 paper, where as the other ones are made up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd7ee0d-7aa4-4f6d-886e-7448e8072897",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cd7ee0d-7aa4-4f6d-886e-7448e8072897",
    "outputId": "bfd8bac7-0091-4f37-eb9f-369a83a587a0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contradiction\n",
      "contradiction\n",
      "neutral\n",
      "entailment\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"“mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity\", model, tokenizer, remove_padding = True))\n",
    "print(sample_from_input_string(\"“mnli premise: John is Simons father. hypothesis: Simon is Johns son\", model, tokenizer, remove_padding = True))\n",
    "print(sample_from_input_string(\"“mnli premise: It is dark at night. hypothesis: After 10 pm it usually gets dark\", model, tokenizer, remove_padding = True))\n",
    "print(sample_from_input_string(\"“mnli premise: Birds have feathers. A pigeon is a bird. hypothesis: A pigeon has feathers\", model, tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be8599-6ddc-49a0-aeed-01488c0e25f7",
   "metadata": {
    "id": "e7be8599-6ddc-49a0-aeed-01488c0e25f7"
   },
   "source": [
    "A similar task is QNLI, which consists of telling whether a given sentence contains the answer to a specific question. Again, the task is made known to the model using a prefix, this time \"qnli\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e09ba4-a050-4455-97f7-13d2d9992b42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60e09ba4-a050-4455-97f7-13d2d9992b42",
    "outputId": "56c1b2e8-e9fe-415d-b77c-9d0e3087730a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entailment\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"qnli question: Where did Jebe die? sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand\", model, tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2e87a-7b9d-464a-8e6a-a8c603e82e05",
   "metadata": {
    "id": "fad2e87a-7b9d-464a-8e6a-a8c603e82e05"
   },
   "source": [
    "A similar task is to not only tell whether the question can be answered based on the information in the context, but also to provide the answer. T5 also has a prefix for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901263e3-f8a5-4eea-916b-a1d40f1b24bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "901263e3-f8a5-4eea-916b-a1d40f1b24bd",
    "outputId": "13d82aa6-0e3d-4cac-c34a-a7396913ec70",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samarkand\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"question: Where did Jebe die? context: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand\", model, tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb98205-6a17-4a2b-a5cf-6184d01633a8",
   "metadata": {
    "id": "1cb98205-6a17-4a2b-a5cf-6184d01633a8"
   },
   "source": [
    "Now let us try something else. We will ask our model an open question, without using any specific prefix. This is called an **instruction** in NLP. So let us ask our model to give us the boiling temperature of water (which is roughly 212 degree F or 100 degree C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e933080-0f75-4ce2-baea-6c79a8dbf577",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e933080-0f75-4ce2-baea-6c79a8dbf577",
    "outputId": "96a162b9-d5ec-48b6-d3fb-cd08fcd62399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitte beantworten Sie folgende Frage: Wie hoch ist die Wasserkochertemperatur?\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"Please answer the following question: what is the boiling temperature of water?\", model, tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd878748-8f74-4813-ba4c-66c77350a64b",
   "metadata": {
    "id": "fd878748-8f74-4813-ba4c-66c77350a64b"
   },
   "source": [
    "Interesting. Apparently our model is not able to derive the actual instruction from the input, but instead mistakenly falls back into a translation of the input. This is not unexpected, as T5 has not been trained specifically on deriving the intent hidden in a prompt, but instead relies on the prefixes to determine the type of task. Two years after T5 was published, Google published a [follow-up paper](https://arxiv.org/abs/2210.11416) in which the team presented FLAN-T5, a version of T5 that, in addition, has been trained on a large number of open questions like in our example. Here, FLAN stands for \"Finetuned language net\".\n",
    "\n",
    "How exactly this was done becomes a bit more tangible by looking at [this code snippet](https://github.com/google-research/FLAN/blob/main/flan/templates.py) which is part of the code used by Google to prepare the dataset used for training.  As we can see, labeled data is converted into different instructions using templates, so that the model learns a large variety of different instructions that refer to the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38252d0d-7b67-4a7f-926c-3f3f691214c2",
   "metadata": {
    "id": "38252d0d-7b67-4a7f-926c-3f3f691214c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version  = \"google/flan-t5-small\"\n",
    "flan_tokenizer = transformers.T5Tokenizer.from_pretrained(model_version, model_max_length=512)\n",
    "flan_model = transformers.T5ForConditionalGeneration.from_pretrained(model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996a419-b0d4-4b6e-99fe-2cf7106aa2b7",
   "metadata": {
    "id": "e996a419-b0d4-4b6e-99fe-2cf7106aa2b7"
   },
   "source": [
    "Let us first try some of our previous example to see how the new model reacts to them and then try our question again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65160fbc-c348-4c2a-991a-727d7780e04d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65160fbc-c348-4c2a-991a-727d7780e04d",
    "outputId": "96cd07b3-75ec-4d33-cbc4-1baba9030fe1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samarkand\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"question: Where did Jebe die? context: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand\", flan_model, flan_tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93e5419e-7626-45a5-a00a-21ae72fb3577",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93e5419e-7626-45a5-a00a-21ae72fb3577",
    "outputId": "59e6bead-4a23-4bf5-cb0e-7834a102112e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Haus ist schön.\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"translate English to German: The house is wonderful.\", flan_model, flan_tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e0e89c-343f-4711-a464-1615af9c2162",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26e0e89c-343f-4711-a464-1615af9c2162",
    "outputId": "fc7fdb79-25d1-4885-e17d-881bb1b7803d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 °C\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"Please answer the following question: what is the boiling temperature of water?\", flan_model, flan_tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acbd6dd-93e2-4366-9f8f-ee09cbb27425",
   "metadata": {
    "id": "4acbd6dd-93e2-4366-9f8f-ee09cbb27425"
   },
   "source": [
    "FLAN T5 comes in different sizes. Let us download the large model (download is a bit more than 3 GB, so this might take some time) and ask it a few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3adae0ff-e573-4712-a476-75169dad7fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3adae0ff-e573-4712-a476-75169dad7fdc",
    "outputId": "4eaba377-c0ad-49a1-bfb1-80e67029b6bf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john glenn\n",
      "212 °F\n"
     ]
    }
   ],
   "source": [
    "model_version  = \"google/flan-t5-large\"\n",
    "flan_tokenizer = transformers.T5Tokenizer.from_pretrained(model_version, model_max_length=512)\n",
    "flan_model = transformers.T5ForConditionalGeneration.from_pretrained(model_version)\n",
    "print(sample_from_input_string(\"Who was the first man in space?\", flan_model, flan_tokenizer, remove_padding = True))\n",
    "print(sample_from_input_string(\"What is the boiling point of water?\", flan_model, flan_tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71772f73-804f-4be7-b515-08b539977f0b",
   "metadata": {
    "id": "71772f73-804f-4be7-b515-08b539977f0b"
   },
   "source": [
    "We see that the model correctly infers the intent of the prompt and gives a reasonable answer. Of course the answer to the first question is factually wrong, but the answer to the second question is correct. Let us try a few questions which were not part of the templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31c6d156-8097-40ce-a537-0526c769059e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31c6d156-8097-40ce-a537-0526c769059e",
    "outputId": "944e280d-3fdb-44f1-ead2-2fa5a1a9f171",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "20 * 2 = 40 years old.\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_input_string(\"True or false: the earth is orbiting around the sun\", flan_model, flan_tokenizer, remove_padding = True))\n",
    "print(sample_from_input_string(\"Pete is 20 years old. Simon is twice as old as Pete. How old is Simon?\", flan_model, flan_tokenizer, remove_padding = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb7045-b378-482e-800c-341f34a9dad6",
   "metadata": {
    "id": "6ccb7045-b378-482e-800c-341f34a9dad6"
   },
   "source": [
    "Not bad! Especially the answer to the second question is impressing. Following FLAN-T5, Google has also applied the same method to other networks, including decoder-only architectures like LamDA-PT, and reports that the models obtained in this way outperforms models trained solely via unsupervised training, see [this paper](https://arxiv.org/pdf/2109.01652v5.pdf)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MLLM",
   "language": "python",
   "name": "mllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
