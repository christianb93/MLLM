{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyEyfDeqmbe8oqyhRqhNAR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will implement the forward method of an LSTM from scratch and then doublecheck against the implementation provided by PyTorch to verify our code. As a starting point, here are the standard formulas for an LSTM cell,.\n",
        "\n",
        "$$\n",
        "f_t = \\sigma(W_{if} x_t  + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
        "g_t = \\tanh(W_{ig} x_t  + b_{ig} +  W_{hg} h_{t-1} + b_{hg})  \\\\\n",
        "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi})    \\\\\n",
        "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})   \\\\\n",
        "c_t = f_t \\\\odot c_{t-1} + i_t \\odot g_t  \\\\\n",
        "h_t = o_t \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "Thus to determine the values of the various gates, we multiply the input $x_t$ with the matrices $W_{ii}, W_{if}, W_{ig}$ and $W_{io}$. Instead of doing this in four separate operations, we can also combine the four matrices into one large matrix of dimension $4 H \\times E$, where $H$ is the dimension of the hidden layer and the cells and $E$ is the dimension of the input, and then carry out one large multiplication. The same holds for the bias and the matrices operating on the hidden state. Let us combine all matrices operating on the input into one matrix $W_{ih}$ and all matrices operating on the hidden state into one matrix $W_{hh}$.\n",
        "\n",
        "Which part of this matrix corresponds to which weight matrix is convention, we will use the approach that PyTorch uses as well under the hood (see  [here](https://github.com/pytorch/pytorch/blob/4130e4f2848ac83baac38dc89d3b95630f39ce7f/torch/nn/modules/rnn.py#L664)). \n",
        "\n",
        "The output that we return will again be the full hidden layer values of shape (L, H) as well as the last value of the hidden layer and the last value of the memory cell, combined into one tuple. We will also allow to pass in existing values for hidden layer and memory cell."
      ],
      "metadata": {
        "id": "3ScBcXr1zOzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "MAoZMvRd0zEe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x, previous_state = None):    \n",
        "    L = x.shape[0]\n",
        "    if previous_state is None:\n",
        "      hidden = torch.zeros(H)\n",
        "      cells = torch.zeros(H)\n",
        "    else:\n",
        "      hidden, cells = previous_state\n",
        "      hidden = hidden.squeeze(dim = 0)\n",
        "      cells = cells.squeeze(dim = 0)\n",
        "    _hidden = []\n",
        "    _cells = []\n",
        "    for i in range(L):\n",
        "        _x = x[i]\n",
        "        #\n",
        "        # multiply w_ih and w_hh by x and h and add biases\n",
        "        # \n",
        "        A = w_ih @ _x \n",
        "        A = A + w_hh @ hidden \n",
        "        A = A + b_ih + b_hh\n",
        "        #\n",
        "        # The value of the forget gate is obtained by taking the second set of H rows of the result\n",
        "        # and applying the sigmoid function\n",
        "        #\n",
        "        ft = torch.sigmoid(A[H:2*H])\n",
        "        #\n",
        "        # Similary the input gate is the first block, the candidate cell the third block and the output gate\n",
        "        # the last block\n",
        "        #\n",
        "        it = torch.sigmoid(A[0:H])\n",
        "        gt = torch.tanh(A[2*H:3*H])\n",
        "        ot = torch.sigmoid(A[3*H:4*H])\n",
        "        #\n",
        "        # New value of cell --> apply forget gate and add input gate times candidate cell\n",
        "        #\n",
        "        cells = ft * cells + it * gt\n",
        "        #\n",
        "        # new value of hidden layer is output gate times cell value\n",
        "        #\n",
        "        hidden = ot * torch.tanh(cells)\n",
        "        _cells.append(cells)\n",
        "        _hidden.append(hidden)\n",
        "    return torch.stack(_hidden), (hidden.unsqueeze(dim = 0), cells.unsqueeze(dim = 0))"
      ],
      "metadata": {
        "id": "JX_jWp0u1f_P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now do a test drive. For that purpose, we will create a PyTorch LSTM, extract the weights from there (I have chosen the way how the weights are modelled in our forward function in alignment with the PyTorch conventions so that this step is easy), run our forward function and the PyTorch network and check that the results are the same."
      ],
      "metadata": {
        "id": "LYe7ydsk8ZaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "E= 5\n",
        "H = 3\n",
        "L = 3\n",
        "#\n",
        "# Create PyTorch LSTM and extract weights\n",
        "#\n",
        "torchLSTM = torch.nn.LSTM(input_size = E, hidden_size = H)\n",
        "w_ih = torchLSTM.weight_ih_l0\n",
        "w_hh = torchLSTM.weight_hh_l0\n",
        "b_ih = torchLSTM.bias_ih_l0\n",
        "b_hh = torchLSTM.bias_hh_l0\n",
        "assert w_ih.shape == (4*H, E), \"Shape of w_ih not as expected\"\n",
        "assert w_hh.shape == (4*H, H), \"Shape of w_hh not as expected\"\n",
        "#\n",
        "# Create random input of dimensions L x E and feed it into\n",
        "# both networks\n",
        "#\n",
        "x = torch.rand(L, E)\n",
        "_out, (_h, _c) = torchLSTM(x)\n",
        "out, (h, c) = forward(x)\n",
        "#\n",
        "# Output will be of shape (L, H)\n",
        "#\n",
        "assert out.shape == (L, H), \"Shape of output not correct\"\n",
        "assert h.shape == (1, H), \"Shape of h not correct\"\n",
        "assert c.shape == (1, H), \"Shape of memory cell not correct\"\n",
        "#\n",
        "# Make sure that outputs match\n",
        "#\n",
        "assert torch.allclose(_out, out), \"Outputs do not match\"\n",
        "assert torch.allclose(_h, h), \"Hidden layers do not match\"\n",
        "assert torch.allclose(_c, c), \"Cells do not match\""
      ],
      "metadata": {
        "id": "B1TTRSEy3-Vb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let us try out the same with a previously obtained hidden and cell state."
      ],
      "metadata": {
        "id": "_loSYQly9Kis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, E)\n",
        "_out, (_h, _c) = torchLSTM(x, (_h, _c))\n",
        "out, (h, c) = forward(x, (h, c))\n",
        "#\n",
        "# Output will be of shape (L, H)\n",
        "#\n",
        "assert out.shape == (1, H), \"Shape of output not correct\"\n",
        "assert h.shape == (1, H), \"Shape of h not correct\"\n",
        "assert c.shape == (1, H), \"Shape of memory cell not correct\"\n",
        "#\n",
        "# Make sure that outputs match\n",
        "#\n",
        "assert torch.allclose(_out, out), \"Outputs do not match\"\n",
        "assert torch.allclose(_h, h), \"Hidden layers do not match\"\n",
        "assert torch.allclose(_c, c), \"Cells do not match\""
      ],
      "metadata": {
        "id": "WjgH41NH9Oxu"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}