{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "483ed6e6-d73a-4c71-8411-c923beb8f905",
   "metadata": {},
   "source": [
    "First, we import the transformer library and load a pretrained GPT-Neo model. This will result in a model which is an instance of *torch.nn.Module*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f8b6b6-bf74-41fb-b0a5-09ad96913f09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chr/Projects/github/MLLM/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 2048)\n",
      "    (wpe): Embedding(2048, 2048)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      ")\n",
      "True\n",
      "GPTNeoConfig {\n",
      "  \"_name_or_path\": \"EleutherAI/gpt-neo-1.3B\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTNeoForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0,\n",
      "  \"attention_layers\": [\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\",\n",
      "    \"global\",\n",
      "    \"local\"\n",
      "  ],\n",
      "  \"attention_types\": [\n",
      "    [\n",
      "      [\n",
      "        \"global\",\n",
      "        \"local\"\n",
      "      ],\n",
      "      12\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embed_dropout\": 0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": null,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neo\",\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"resid_dropout\": 0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 0.9\n",
      "    }\n",
      "  },\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"transformers_version\": \"4.27.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257,\n",
      "  \"window_size\": 256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "model_name=\"EleutherAI/gpt-neo-1.3B\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(model)\n",
    "print(isinstance(model, torch.nn.Module))\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f91c00-61a8-4a13-bd4f-7434f255c7a1",
   "metadata": {},
   "source": [
    "Next we need a matching tokenizer which will also act as the encoder. We can generate a tokenizer using *AutoTokenizer.from_pretrained*. We will see that apart from the vocabulary and methods to encode and decode, the tokenizer also contains information on special token which in general should match that in the model configuration *model.config*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc619b2-0c5d-489c-a8c7-f7b336c4238b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='EleutherAI/gpt-neo-1.3B', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81cbe2f-ce4f-4c69-92e2-eea6fef5b17a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50257\n",
      "UNK token: <|endoftext|> (ID = 50256)\n",
      "BOS token: <|endoftext|> (ID = 50256)\n",
      "EOS token: <|endoftext|> (ID = 50256)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"UNK token: {tokenizer.unk_token} (ID = {tokenizer.unk_token_id})\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} (ID = {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} (ID = {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023134da-de3c-4013-bb31-6eb25c681a72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 257, 1790, 1332]\n",
      "Token ID 1212 is This\n",
      "Token ID 318 is Ġis\n",
      "Token ID 257 is Ġa\n",
      "Token ID 1790 is Ġshort\n",
      "Token ID 1332 is Ġtest\n",
      "Result of decoding: This is a short test\n",
      "{'input_ids': [1212, 318, 257, 1790, 1332], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "test = \"This is a short test\"\n",
    "ids = tokenizer.encode(test)\n",
    "print(ids)\n",
    "for id in ids:\n",
    "    print(f\"Token ID {id} is {tokenizer.convert_ids_to_tokens(id)}\")#\n",
    "print(f\"Result of decoding: {tokenizer.decode(ids)}\")\n",
    "print(tokenizer(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338d7cd-2abb-499d-90e2-0a1c59ddbb90",
   "metadata": {},
   "source": [
    "Knowing how to decode and encode, we can now write a simple function to continue a prompt, as we have done it for our own transformer trained on the Wikipedia data set. The only difference that we have to observe is that the output of the model is not simply the logits, but a dictionary containing the logits and the past key values (we get to this point later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6960c1-58a0-41fa-a814-6fa8542f65bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'past_key_values'])\n",
      "torch.Size([5, 50257])\n",
      "24\n",
      "torch.Size([1, 16, 5, 128])\n",
      "torch.Size([1, 16, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = model(torch.tensor(ids))\n",
    "print(out.keys())\n",
    "print(out.logits.shape)\n",
    "#\n",
    "# Past key values is an array with one entry per layer\n",
    "# For each layer, the shape is B x H x L x head_dim\n",
    "# see https://github.com/huggingface/transformers/blob/4e9f6fc67ce6290b3ab6efe2ddb1fcfc3e554382/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L230\n",
    "#\n",
    "print(len(out.past_key_values))\n",
    "print(out.past_key_values[0][0].shape) # keys\n",
    "print(out.past_key_values[0][1].shape) # values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc86f222-2181-4ec6-adba-8d04eaba2be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_p_sampling(p, p_val = 0.95):\n",
    "    #\n",
    "    # Apply top-p sampling (nucleus sampling)\n",
    "    #\n",
    "    items , indices = torch.sort(p, descending = True)    \n",
    "    _k = max((torch.cumsum(items, dim = 0) <= p_val).to(int).sum().item(), 1)\n",
    "    keep = indices[:_k]\n",
    "    _p = [p[i] for i in keep]\n",
    "    idx = torch.distributions.categorical.Categorical(probs = torch.tensor(_p)).sample().item()\n",
    "    idx = keep[idx]\n",
    "    return idx.item()\n",
    "\n",
    "    \n",
    "def predict(model, prompt, length, tokenizer, temperature = 0.7,  p_val = 0.95):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample = []\n",
    "        device = next(model.parameters()).device\n",
    "        #\n",
    "        # Turn prompt into sequence of token IDs\n",
    "        # \n",
    "        encoded_prompt  = tokenizer.encode(prompt)        \n",
    "        encoded_sample = encoded_prompt\n",
    "        encoded_prompt = torch.tensor(encoded_prompt, dtype = torch.long).unsqueeze(dim = 0)\n",
    "        with torch.no_grad():\n",
    "            out = model(encoded_prompt.to(device)).logits # shape B x L x V\n",
    "            while (len(encoded_sample) < length):\n",
    "                #\n",
    "                # Sample next character from last output. Note that we need to remove the\n",
    "                # batch dimension to obtain shape (L, V) and take the last element only\n",
    "                #\n",
    "                p = torch.nn.functional.softmax(out[0, -1, :] / temperature, dim = -1)\n",
    "                #\n",
    "                # Sample new index and append to encoded sample\n",
    "                #\n",
    "                encoded_sample.append(do_p_sampling(p, p_val))\n",
    "                #\n",
    "                # Feed new sequence\n",
    "                #\n",
    "                input = torch.tensor(encoded_sample[-model.config.max_position_embeddings:], dtype=torch.long)\n",
    "                input = torch.unsqueeze(input, dim = 0)\n",
    "                out = model(input.to(device)).logits\n",
    "                print(tokenizer.decode(encoded_sample))\n",
    "\n",
    "        return tokenizer.decode(encoded_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e8eea6-6981-4b98-8ada-aefed0ed5649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Joe and I am a\n",
      "My name is Joe and I am a professional\n",
      "My name is Joe and I am a professional baseball\n",
      "My name is Joe and I am a professional baseball player\n",
      "My name is Joe and I am a professional baseball player for\n",
      "My name is Joe and I am a professional baseball player for the\n",
      "My name is Joe and I am a professional baseball player for the St\n",
      "My name is Joe and I am a professional baseball player for the St.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My name is Joe and I am a professional baseball player for the St.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model = model, prompt = \"My name is Joe and I am\", length = 15, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f6631cc-29cc-4fae-9d9c-f8cf500872a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A long time ago\"\n",
    "encoded_prompt = tokenizer.encode(prompt)\n",
    "input_ids = torch.tensor(encoded_prompt).unsqueeze(dim = 0)\n",
    "print(input_ids.shape) # B x L where B = 1 and L = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571c93a8-f4a7-458d-86f5-837b353abfe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 50257])\n",
      "torch.Size([1, 16, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "out = model(input_ids = input_ids)\n",
    "logits = out.logits\n",
    "print(logits.shape) # B x L x V\n",
    "past_key_values = out.past_key_values\n",
    "print(past_key_values[0][0].shape) # B x H x L x head_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c2f6c46-3b76-4932-8e5b-0bd8b1f4f8be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 50257])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# First append new token and run entire new sample through model. It does not matter which token we choose\n",
    "#\n",
    "encoded_sample = encoded_prompt\n",
    "new_id = tokenizer.convert_tokens_to_ids(\",\")\n",
    "encoded_sample.append(new_id)\n",
    "input_ids = torch.tensor(encoded_sample).unsqueeze(dim = 0)\n",
    "out1 = model(input_ids = input_ids)\n",
    "print(out1.logits.shape) # B x (L + 1) x V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e138b6-b114-4c2b-a851-da3ecf47c42d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n",
      "torch.Size([1, 16, 5, 128])\n",
      "tensor(-9.7990, grad_fn=<SelectBackward0>)\n",
      "tensor(-9.7990, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Now try the same with the past key values\n",
    "#\n",
    "out2 = model(input_ids = torch.tensor(new_id).unsqueeze(dim = 0), past_key_values = past_key_values)\n",
    "print(out2.logits.shape) # B x V\n",
    "#\n",
    "# the model will be kind and return the full past keys and values again - shape B x H x (L+1) x head_dim\n",
    "# \n",
    "print(out2.past_key_values[0][0].shape) \n",
    "print(out2.logits[0, 1])\n",
    "print(out1.logits[0, -1, 1])\n",
    "V = model.config.vocab_size\n",
    "assert torch.allclose(out1.logits[0, -1, :], out2.logits[0], rtol = 1e-2), \"Logits do not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76488c58-afc1-4e98-a070-d010d22d739e",
   "metadata": {},
   "source": [
    "Let us now use this pattern for a new, significantly more efficient prediction method. When you run this, you will find that the first pass through the model still takes some time, but the remaining passes are much faster and bearable, even on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee1c01a5-4e78-4f72-a35d-fe6a60cdc628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, prompt, length, tokenizer, temperature = 0.7,  p_val = 0.95):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample = []\n",
    "        device = next(model.parameters()).device\n",
    "        #\n",
    "        # Turn prompt into sequence of token IDs\n",
    "        # \n",
    "        encoded_prompt  = tokenizer.encode(prompt)        \n",
    "        encoded_sample = encoded_prompt\n",
    "        input_ids = torch.tensor(encoded_prompt, dtype = torch.long).unsqueeze(dim = 0)\n",
    "        with torch.no_grad():\n",
    "            #\n",
    "            # First forward pass- use full prompt\n",
    "            #\n",
    "            out = model(input_ids = input_ids.to(device))\n",
    "            logits = out.logits[:, -1, :] # shape B x V\n",
    "            past_key_values = out.past_key_values\n",
    "            while (len(encoded_sample) < length):\n",
    "                #\n",
    "                # Sample next character from last output. Note that we need to remove the\n",
    "                # batch dimension to obtain shape (L, V) and take the last element only\n",
    "                #\n",
    "                p = torch.nn.functional.softmax(logits[0, :] / temperature, dim = -1)\n",
    "                #\n",
    "                # Sample new index and append to encoded sample\n",
    "                #\n",
    "                idx = do_p_sampling(p, p_val)\n",
    "                encoded_sample.append(idx)\n",
    "                #\n",
    "                # Feed new sequence\n",
    "                #\n",
    "                input_ids = torch.tensor(idx).unsqueeze(dim = 0)\n",
    "                out = model(input_ids = input_ids.to(device), past_key_values = past_key_values)\n",
    "                logits = out.logits\n",
    "                past_key_values = out.past_key_values\n",
    "                print(tokenizer.decode(encoded_sample))\n",
    "\n",
    "        return tokenizer.decode(encoded_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e03d511-bf12-4038-b644-da84b2a4b5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Joe and I am a\n",
      "My name is Joe and I am a senior\n",
      "My name is Joe and I am a senior in\n",
      "My name is Joe and I am a senior in high\n",
      "My name is Joe and I am a senior in high school\n",
      "My name is Joe and I am a senior in high school.\n",
      "My name is Joe and I am a senior in high school. I\n",
      "My name is Joe and I am a senior in high school. I am\n",
      "My name is Joe and I am a senior in high school. I am not\n",
      "My name is Joe and I am a senior in high school. I am not a\n",
      "My name is Joe and I am a senior in high school. I am not a gang\n",
      "My name is Joe and I am a senior in high school. I am not a gang member\n",
      "My name is Joe and I am a senior in high school. I am not a gang member.\n",
      "My name is Joe and I am a senior in high school. I am not a gang member. I\n",
      "My name is Joe and I am a senior in high school. I am not a gang member. I am\n",
      "My name is Joe and I am a senior in high school. I am not a gang member. I am not\n",
      "My name is Joe and I am a senior in high school. I am not a gang member. I am not a\n",
      "My name is Joe and I am a senior in high school. I am not a gang member. I am not a murderer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My name is Joe and I am a senior in high school. I am not a gang member. I am not a murderer'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model = model, prompt = \"My name is Joe and I am\", length = 25, tokenizer = tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLLM",
   "language": "python",
   "name": "mllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
